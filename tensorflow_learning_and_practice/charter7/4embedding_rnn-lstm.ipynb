{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.18.1\n",
      "pandas 1.0.0\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "vocab_size = 10000\n",
    "index_from = 3\n",
    "\n",
    "# 载入数据\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from) # num_words设置词表数量，前vocab_size个会保留下来，后面的当成特殊字符处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0], train_labels[0])\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(len(train_data[0]), len(train_data[1])) # 变长数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入词表\n",
    "word_index = imdb.get_word_index()\n",
    "print(len(word_index))\n",
    "print(type(word_index))\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {k:(v+3) for k, v in word_index.items()}  # word_index中每个词的index都增加3(因为index_from设为3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前几个index用作特殊字符\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['END'] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for key, value in word_index.items()])\n",
    "\n",
    "def decode_review(text_ids):\n",
    "    return ' '.join([reverse_word_index.get(word_id, '<UNK>') for word_id in text_ids])\n",
    "\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据补全和截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
    "                                                        value=word_index['<PAD>'],  # 填充的值\n",
    "                                                        padding='post', # padding = ['post', 'pre'] post: 把padding放在句子后面，pre: 把padding放在句子前面\n",
    "                                                        maxlen = max_length\n",
    "                                                        )\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, \n",
    "                                                        value=word_index['<PAD>'],  # 填充的值\n",
    "                                                        padding='post', # padding = ['post', 'pre'] post: 把padding放在句子后面，pre: 把padding放在句子前面\n",
    "                                                        maxlen = max_length\n",
    "                                                        )\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "model = keras.models.Sequential([\n",
    "    # embedding层：\n",
    "    # 1.定义一个矩阵[vacab_size, embedding_dim] 来定义每个词的词向量\n",
    "    # 2.对于每个句子分别对每个词做embedding -> [batch_size, max_length, embedding_dim]的一个矩阵\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    # # 单层单向RNN\n",
    "    # keras.layers.LSTM(units=64, return_sequences=False),   # return_sequence: 最后一个RNN设False，否则设True\n",
    "    # 多层双向RNN\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=True)), # 双向RNN\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(units=64, return_sequences=False)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)  # validation_split多少比例的训练数据被当做验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data['val_'+label] = history.history['val_'+label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history, 'accuracy', epochs, 0, 1.5)\n",
    "plot_learning_curves(history, 'loss', epochs, 0, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
